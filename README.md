# LLMUI Core v0.5.0

[![AGPL v3](https://img.shields.io/badge/AGPL%20v3-Open%20Source-green.svg)](https://www.gnu.org/licenses/agpl-3.0)
[![Commons Clause](https://img.shields.io/badge/Commons%20Clause-No%20Commercial-red.svg)](LICENSE)
[![Enterprise Clause](https://img.shields.io/badge/Enterprise-Publication%20Required-orange.svg)](ENTERPRISE_CLAUSE_EXPLAINED.md)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![Platform](https://img.shields.io/badge/Platform-Linux-lightgrey.svg)](https://kernel.org)

> **Plateforme de consensus multi-modÃ¨les IA avec souverainetÃ© numÃ©rique**

DÃ©veloppÃ© par **GÃ©nie IA Centre OpÃ©rationnel SÃ©curitÃ© inc.** - Une solution quÃ©bÃ©coise pour l'intelligence artificielle locale et Ã©thique.

---

## ğŸ¯ Vue d'ensemble

LLMUI Core est une plateforme innovante de consensus entre plusieurs modÃ¨les de langage (LLM), permettant de:

- **Orchestrer plusieurs modÃ¨les IA** en parallÃ¨le (workers + mergers)
- **Obtenir des rÃ©ponses par consensus** pour une qualitÃ© supÃ©rieure
- **Garantir la souverainetÃ© numÃ©rique** - hÃ©bergement local, sans cloud
- **IntÃ©grer des systÃ¨mes de mÃ©moire avancÃ©s** (RAG, hybride)
- **Traiter des fichiers** (PDF, DOCX, images, etc.)
- **Maintenir l'historique complet** avec SQLite

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx     â”‚ â† Interface web (port 80/443)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ llmui-proxy â”‚ â† Gestion sessions, auth (port 8080)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚llmui-backendâ”‚ â† Orchestration LLM (port 5000)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama    â”‚ â† ModÃ¨les locaux (port 11434)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â€¢ phi3:3.8b (worker)
  â€¢ gemma2:2b (worker)
  â€¢ granite4:micro-h (merger)
```

---

## ğŸ“‹ PrÃ©requis

### MatÃ©riel recommandÃ©
- **CPU**: 4 cÅ“urs minimum, 8+ recommandÃ©
- **RAM**: 8GB minimum, 16GB+ recommandÃ©
- **Disque**: 20GB minimum, 50GB+ recommandÃ©
- **GPU**: Optionnel mais amÃ©liore les performances

### SystÃ¨me d'exploitation
- Debian 11/12
- Ubuntu 20.04/22.04/24.04
- Rocky Linux 8/9
- RHEL 8/9

### Logiciels
- Python 3.8+
- AccÃ¨s root (sudo)
- Git

---

## ğŸš€ Installation rapide avec Andy

**Andy** est l'assistant DevOps autonome qui automatise l'installation complÃ¨te de LLMUI Core.

### Installation en une commande

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/votre-repo/llmui-core.git
cd llmui-core

# Lancer l'installation interactive
sudo bash andy_setup.sh
```

### Installation complÃ¨te automatique

```bash
# Installation en 3 Ã©tapes automatisÃ©es
sudo python3 andy_installer.py      # Ã‰tape 1: Base systÃ¨me
sudo python3 andy_deploy_source.py  # Ã‰tape 2: Fichiers source
sudo python3 andy_start_services.py # Ã‰tape 3: Services
```

Andy va automatiquement:
- âœ… Mettre Ã  jour l'OS
- âœ… Installer Ollama + 3 modÃ¨les LLM
- âœ… Configurer Python + dÃ©pendances
- âœ… CrÃ©er les services systemd
- âœ… Configurer Nginx + SSL
- âœ… Configurer le pare-feu
- âœ… VÃ©rifier l'installation

> **Note**: Andy vous demandera uniquement le nom d'utilisateur et le mot de passe pour l'interface LLMUI.

### Ce que fait Andy

1. **Installation de base** (`andy_installer.py`)
   - DÃ©tection automatique du systÃ¨me (apt/dnf/yum)
   - Installation des dÃ©pendances systÃ¨me
   - Installation d'Ollama et tÃ©lÃ©chargement des modÃ¨les
   - CrÃ©ation de l'environnement virtuel Python
   - Configuration des services systemd
   - Configuration Nginx et pare-feu

2. **DÃ©ploiement des sources** (`andy_deploy_source.py`)
   - Clone du dÃ©pÃ´t Git (ou copie manuelle)
   - Installation des fichiers dans `/opt/llmui-core/`
   - Configuration des permissions

3. **DÃ©marrage des services** (`andy_start_services.py`)
   - Activation des services systemd
   - DÃ©marrage backend â†’ proxy â†’ nginx
   - VÃ©rification de l'Ã©tat des services
   - Test HTTP et affichage de l'URL d'accÃ¨s

### Menu interactif (andy_setup.sh)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           MENU PRINCIPAL                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [1] Installation complÃ¨te (recommandÃ©)
  [2] Installation de base uniquement
  [3] DÃ©ployer les fichiers source
  [4] DÃ©marrer les services
  [5] VÃ©rifier l'installation
  [6] Consulter les logs
  [7] Lire la documentation
  [Q] Quitter
```

---

## ğŸ“¦ Structure du projet

```
llmui-core/
â”œâ”€â”€ andy_setup.sh              # Menu interactif
â”œâ”€â”€ andy_installer.py          # Installation base systÃ¨me
â”œâ”€â”€ andy_deploy_source.py      # DÃ©ploiement sources
â”œâ”€â”€ andy_start_services.py     # DÃ©marrage services
â”œâ”€â”€ README.md                  # Ce fichier
â”œâ”€â”€ README_ANDY.md             # Documentation Andy
â”œâ”€â”€ INSTALL.md                 # Guide installation dÃ©taillÃ©
â”œâ”€â”€ LICENSE                    # Licence propriÃ©taire
â”‚
â”œâ”€â”€ src/                       # Code source backend
â”‚   â”œâ”€â”€ llmui_backend.py      # Serveur FastAPI principal
â”‚   â”œâ”€â”€ llmui_proxy.py        # Serveur proxy
â”‚   â”œâ”€â”€ auth.py               # Authentification
â”‚   â”œâ”€â”€ database.py           # Gestion SQLite
â”‚   â”œâ”€â”€ memory.py             # SystÃ¨me mÃ©moire
â”‚   â””â”€â”€ file_processor.py     # Traitement fichiers
â”‚
â”œâ”€â”€ web/                       # Interface web
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ css/
â”‚   â”œâ”€â”€ js/
â”‚   â””â”€â”€ assets/
â”‚
â”œâ”€â”€ config.yaml                # Configuration principale
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”‚
â””â”€â”€ docs/                      # Documentation
    â”œâ”€â”€ ARCHITECTURE.md
    â”œâ”€â”€ API.md
    â”œâ”€â”€ CONFIGURATION.md
    â””â”€â”€ TROUBLESHOOTING.md
```

---

## ğŸ”§ Configuration

### Fichier principal: `config.yaml`

```yaml
server:
  host: "0.0.0.0"
  port: 5000
  ssl_enabled: false

ollama:
  base_url: "http://localhost:11434"
  models:
    workers:
      - "phi3:3.8b"
      - "gemma2:2b"
    merger: "granite4:micro-h"

database:
  path: "/opt/llmui-core/data/llmui.db"

security:
  jwt_secret: "auto-generated"
  session_timeout: 3600
```

### Ports utilisÃ©s

| Service | Port | Description |
|---------|------|-------------|
| Nginx | 80/443 | Interface web |
| llmui-proxy | 8080 | Proxy + auth |
| llmui-backend | 5000 | API backend |
| Ollama | 11434 | Serveur LLM |

---

## ğŸ® Utilisation

### AccÃ¨s Ã  l'interface

Une fois installÃ©, accÃ©dez Ã  LLMUI Core via votre navigateur:

```
http://VOTRE_IP/
```

L'IP du serveur est affichÃ©e Ã  la fin de l'installation par Andy.

### Gestion des services

```bash
# Statut des services
sudo systemctl status llmui-backend
sudo systemctl status llmui-proxy
sudo systemctl status nginx

# RedÃ©marrer les services
sudo systemctl restart llmui-backend
sudo systemctl restart llmui-proxy
sudo systemctl restart nginx

# Logs en temps rÃ©el
sudo journalctl -u llmui-backend -f
sudo journalctl -u llmui-proxy -f
```

### API REST

Documentation complÃ¨te de l'API disponible dans [`docs/API.md`](docs/API.md).

Exemples d'endpoints:

```bash
# Health check
curl http://localhost:5000/api/health

# Liste des modÃ¨les
curl http://localhost:5000/api/models

# Nouvelle conversation
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Bonjour!", "user_id": "user123"}'
```

---

## ğŸ“Š FonctionnalitÃ©s

### âœ¨ Consensus multi-modÃ¨les

LLMUI Core utilise une approche unique:
1. **Workers analysent** le prompt en parallÃ¨le
2. **Merger synthÃ©tise** les rÃ©ponses en consensus
3. **QualitÃ© supÃ©rieure** grÃ¢ce Ã  la diversitÃ© des modÃ¨les

### ğŸ§  SystÃ¨me de mÃ©moire avancÃ©

- **MÃ©moire court terme**: Contexte de conversation
- **MÃ©moire long terme**: SQLite avec recherche sÃ©mantique
- **RAG (Retrieval-Augmented Generation)**: Base vectorielle
- **MÃ©moire hybride**: Combinaison intelligente

### ğŸ“ Traitement de fichiers

Formats supportÃ©s:
- **Documents**: PDF, DOCX, TXT, MD
- **Images**: PNG, JPG, WEBP
- **DonnÃ©es**: CSV, JSON, YAML
- **Code**: Python, JavaScript, etc.

### ğŸ” SÃ©curitÃ©

- **Authentification JWT**
- **Chiffrement des sessions**
- **Pare-feu configurÃ©**
- **Headers de sÃ©curitÃ© Nginx**
- **Isolation des services**
- **Permissions strictes**

---

## ğŸ“– Documentation complÃ¨te

- **[INSTALL.md](INSTALL.md)** - Guide d'installation dÃ©taillÃ©
- **[README_ANDY.md](README_ANDY.md)** - Documentation Andy
- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** - Architecture technique
- **[API.md](docs/API.md)** - Documentation API REST
- **[CONFIGURATION.md](docs/CONFIGURATION.md)** - Configuration avancÃ©e
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - DÃ©pannage

---

## ğŸ› DÃ©pannage rapide

### Les services ne dÃ©marrent pas

```bash
# VÃ©rifier les logs
sudo journalctl -u llmui-backend -n 50
sudo journalctl -u llmui-proxy -n 50

# VÃ©rifier les permissions
ls -la /opt/llmui-core/

# VÃ©rifier l'environnement Python
/opt/llmui-core/venv/bin/python --version
/opt/llmui-core/venv/bin/pip list
```

### Nginx erreur 502

Le backend n'est pas dÃ©marrÃ©:
```bash
sudo systemctl status llmui-backend
sudo systemctl start llmui-backend
```

### Ollama ne rÃ©pond pas

```bash
ollama list
ollama ps
sudo systemctl status ollama
sudo systemctl restart ollama
```

### Consulter les logs d'Andy

```bash
# Log d'installation
less /tmp/andy_install.log

# Base de donnÃ©es SQLite
sqlite3 /tmp/andy_installation.db
```

---

## ğŸ”„ Mise Ã  jour

```bash
# Sauvegarder la configuration
sudo cp /opt/llmui-core/config.yaml /opt/llmui-core/config.yaml.bak

# ArrÃªter les services
sudo systemctl stop llmui-backend llmui-proxy

# Mettre Ã  jour le code
cd /path/to/llmui-core
git pull origin main

# RedÃ©ployer
sudo python3 andy_deploy_source.py

# RedÃ©marrer
sudo python3 andy_start_services.py
```

---

## ğŸ¤ Contribution

Ce projet est dÃ©veloppÃ© par **Francois Chalut**

Pour toute question ou contribution:
- **Email**: contact@llmui.org
- **Issues**: [GitHub Issues](https://github.com/GenAICos/llmui-core/issues)

---

## ğŸ“œ Licence

Â© 2025 Francois Chalut.

AGPLv3 + common clause

Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸŒŸ Philosophie du projet

LLMUI Core s'inscrit dans une vision de **souverainetÃ© numÃ©rique quÃ©bÃ©coise**:

- ğŸ‡¨ğŸ‡¦ **Local d'abord**: HÃ©bergement et contrÃ´le complets
- ğŸ”“ **Open Architecture**: Extensible et adaptable
- ğŸ›¡ï¸ **SÃ©curitÃ© par conception**: Protection des donnÃ©es
- ğŸ¤– **IA Ã©thique**: Transparence et consensus
- ğŸŒ± **Autonomie technologique**: IndÃ©pendance des GAFAM

---

## ğŸ“ Support

**Documentation**: [GitHub Wiki](https://github.com/GenAICos/llmui-core/wiki)  
**Logs Andy**: `/tmp/andy_install.log`  
**Base de donnÃ©es Andy**: `/tmp/andy_installation.db`  
**Installation**: `/opt/llmui-core/`

---

**DÃ©veloppÃ© avec ğŸ’™ au QuÃ©bec par Francois Chalut**
