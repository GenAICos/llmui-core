#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
caching_system.py - Multi-Level Caching System
Cache intelligent avec fallback automatique

Author: FranÃ§ois Chalut
Website: https://llmui.org

ARCHITECTURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L1: Memory Cache (LRU) - Ultra rapide     â”‚  <-- VÃ©rifie d'abord
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L2: Redis - PartagÃ© entre instances       â”‚  <-- Si L1 miss
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L3: Ollama - GÃ©nÃ©ration rÃ©elle            â”‚  <-- Si L1+L2 miss
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FEATURES:
- Cache automatique des rÃ©ponses
- TTL configurable
- Invalidation intelligente
- Fallback automatique si Redis down
- Metrics intÃ©grÃ©s

PERFORMANCE:
- L1 hit: ~0.1ms (1000x plus rapide)
- L2 hit: ~2ms (100x plus rapide)
- L3 miss: ~5000ms (gÃ©nÃ©ration complÃ¨te)

Usage:
    cache = CacheSystem()
    
    # Automatique
    response = await cache.get_or_generate(
        model="qwen2.5:8b",
        prompt="What is Python?",
        generator=lambda: call_ollama(...)
    )
    
    # Stats
    print(cache.get_stats())
"""

import hashlib
import json
import logging
import time
from collections import OrderedDict
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, Optional, Union

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    logging.warning("âš ï¸  Redis not installed. L2 cache disabled. Install: pip install redis")

# ============================================================================
# CONFIGURATION
# ============================================================================

logger = logging.getLogger(__name__)

@dataclass
class CacheConfig:
    """Configuration du systÃ¨me de cache"""
    
    # L1 Cache (Memory)
    memory_cache_size: int = 1000
    memory_ttl_seconds: int = 3600  # 1 heure
    
    # L2 Cache (Redis)
    redis_enabled: bool = True
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0
    redis_password: Optional[str] = None
    redis_ttl_seconds: int = 86400  # 24 heures
    redis_prefix: str = "llmui"
    
    # GÃ©nÃ©ral
    cache_enabled: bool = True
    min_prompt_length: int = 10  # Ne pas cacher les prompts trop courts
    cache_embeddings: bool = True
    cache_chat: bool = True
    
    # Compression (optionnel)
    compress_responses: bool = False
    compression_threshold: int = 1024  # Compresser si > 1KB

# ============================================================================
# CACHE KEY GENERATOR
# ============================================================================

class CacheKeyGenerator:
    """GÃ©nÃ¨re des clÃ©s de cache uniques et dÃ©terministes"""
    
    @staticmethod
    def generate(
        model: str,
        prompt: str,
        options: Optional[Dict] = None,
        prefix: str = "llmui"
    ) -> str:
        """
        GÃ©nÃ¨re une clÃ© de cache unique
        
        Args:
            model: Nom du modÃ¨le
            prompt: Le prompt
            options: Options supplÃ©mentaires (temperature, etc.)
            prefix: PrÃ©fixe pour la clÃ©
        
        Returns:
            ClÃ© de cache unique
        """
        # CrÃ©er une signature unique
        signature = {
            "model": model,
            "prompt": prompt,
            "options": options or {}
        }
        
        # SÃ©rialiser en JSON (triÃ©e pour cohÃ©rence)
        json_str = json.dumps(signature, sort_keys=True)
        
        # Hash SHA256
        hash_obj = hashlib.sha256(json_str.encode('utf-8'))
        hash_hex = hash_obj.hexdigest()[:16]  # 16 premiers caractÃ¨res
        
        # Format: llmui:model:hash
        return f"{prefix}:{model}:{hash_hex}"
    
    @staticmethod
    def should_cache(prompt: str, config: CacheConfig) -> bool:
        """DÃ©termine si un prompt devrait Ãªtre cachÃ©"""
        if not config.cache_enabled:
            return False
        
        if len(prompt) < config.min_prompt_length:
            return False
        
        # Ne pas cacher les prompts avec des timestamps ou IDs uniques
        unique_markers = ["timestamp", "uuid", "current_time", "random"]
        prompt_lower = prompt.lower()
        
        for marker in unique_markers:
            if marker in prompt_lower:
                return False
        
        return True

# ============================================================================
# L1: MEMORY CACHE (LRU)
# ============================================================================

class MemoryCache:
    """
    Cache mÃ©moire LRU (Least Recently Used)
    
    Features:
    - Suppression automatique des vieux items
    - TTL par item
    - Thread-safe
    """
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache: OrderedDict = OrderedDict()
        self.timestamps: Dict[str, float] = {}
        
        self.hits = 0
        self.misses = 0
        
        logger.info(f"âœ… L1 Memory cache initialized: {max_size} items, {ttl_seconds}s TTL")
    
    def get(self, key: str) -> Optional[str]:
        """RÃ©cupÃ¨re une valeur du cache"""
        if key not in self.cache:
            self.misses += 1
            return None
        
        # VÃ©rifier TTL
        if self._is_expired(key):
            self.delete(key)
            self.misses += 1
            return None
        
        # LRU: dÃ©placer Ã  la fin
        self.cache.move_to_end(key)
        self.hits += 1
        
        return self.cache[key]
    
    def set(self, key: str, value: str):
        """Stocke une valeur dans le cache"""
        # Si dÃ©jÃ  prÃ©sent, mettre Ã  jour
        if key in self.cache:
            self.cache.move_to_end(key)
        
        # Ajouter
        self.cache[key] = value
        self.timestamps[key] = time.time()
        
        # Supprimer le plus vieux si trop plein
        if len(self.cache) > self.max_size:
            oldest_key = next(iter(self.cache))
            self.delete(oldest_key)
    
    def delete(self, key: str):
        """Supprime une clÃ© du cache"""
        self.cache.pop(key, None)
        self.timestamps.pop(key, None)
    
    def clear(self):
        """Vide le cache"""
        count = len(self.cache)
        self.cache.clear()
        self.timestamps.clear()
        logger.info(f"L1 cache cleared: {count} items removed")
    
    def _is_expired(self, key: str) -> bool:
        """VÃ©rifie si une clÃ© est expirÃ©e"""
        if key not in self.timestamps:
            return True
        
        age = time.time() - self.timestamps[key]
        return age > self.ttl_seconds
    
    def stats(self) -> Dict:
        """Statistiques du cache"""
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0
        
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": f"{hit_rate:.1f}%",
            "usage": f"{len(self.cache)}/{self.max_size}"
        }

# ============================================================================
# L2: REDIS CACHE
# ============================================================================

class RedisCache:
    """
    Cache Redis partagÃ© entre instances
    
    Features:
    - PartagÃ© entre processus/serveurs
    - Persistance
    - TTL automatique
    - Fallback si indisponible
    """
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.redis_client = None
        self.is_available = False
        
        self.hits = 0
        self.misses = 0
        
        if not config.redis_enabled:
            logger.info("â© L2 Redis cache disabled by config")
            return
        
        if not REDIS_AVAILABLE:
            logger.warning("âš ï¸  L2 Redis cache unavailable: redis module not installed")
            return
        
        self._connect()
    
    def _connect(self):
        """Connexion Ã  Redis"""
        try:
            self.redis_client = redis.Redis(
                host=self.config.redis_host,
                port=self.config.redis_port,
                db=self.config.redis_db,
                password=self.config.redis_password,
                decode_responses=True,
                socket_timeout=2,
                socket_connect_timeout=2
            )
            
            # Test connexion
            self.redis_client.ping()
            self.is_available = True
            
            logger.info(f"âœ… L2 Redis cache connected: {self.config.redis_host}:{self.config.redis_port}")
            
        except Exception as e:
            logger.warning(f"âš ï¸  L2 Redis cache unavailable: {e}")
            self.is_available = False
    
    def get(self, key: str) -> Optional[str]:
        """RÃ©cupÃ¨re une valeur de Redis"""
        if not self.is_available:
            return None
        
        try:
            value = self.redis_client.get(key)
            
            if value:
                self.hits += 1
                return value
            else:
                self.misses += 1
                return None
                
        except Exception as e:
            logger.warning(f"Redis get error: {e}")
            self.misses += 1
            return None
    
    def set(self, key: str, value: str, ttl: Optional[int] = None):
        """Stocke une valeur dans Redis"""
        if not self.is_available:
            return
        
        try:
            ttl = ttl or self.config.redis_ttl_seconds
            self.redis_client.setex(key, ttl, value)
            
        except Exception as e:
            logger.warning(f"Redis set error: {e}")
            self.is_available = False
    
    def delete(self, key: str):
        """Supprime une clÃ© de Redis"""
        if not self.is_available:
            return
        
        try:
            self.redis_client.delete(key)
        except Exception as e:
            logger.warning(f"Redis delete error: {e}")
    
    def clear(self, pattern: str = "llmui:*"):
        """Vide le cache par pattern"""
        if not self.is_available:
            return
        
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                self.redis_client.delete(*keys)
            logger.info(f"L2 cache cleared: {len(keys)} keys matching '{pattern}'")
        except Exception as e:
            logger.warning(f"Redis clear error: {e}")
    
    def stats(self) -> Dict:
        """Statistiques Redis"""
        if not self.is_available:
            return {"available": False}
        
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0
        
        try:
            info = self.redis_client.info("stats")
            return {
                "available": True,
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": f"{hit_rate:.1f}%",
                "redis_keys": self.redis_client.dbsize(),
                "redis_memory": info.get("used_memory_human", "unknown"),
            }
        except:
            return {
                "available": True,
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": f"{hit_rate:.1f}%",
            }

# ============================================================================
# MAIN CACHE SYSTEM
# ============================================================================

class CacheSystem:
    """
    SystÃ¨me de cache multi-niveau avec fallback automatique
    
    Usage:
        cache = CacheSystem()
        
        # Automatique avec gÃ©nÃ©rateur
        response = cache.get_or_generate(
            model="qwen2.5:8b",
            prompt="What is Python?",
            generator=lambda: call_ollama(...)
        )
        
        # Manuel
        cached = cache.get(model, prompt)
        if not cached:
            result = generate()
            cache.set(model, prompt, result)
    """
    
    def __init__(self, config: Optional[CacheConfig] = None):
        self.config = config or CacheConfig()
        
        # Initialiser les caches
        self.l1 = MemoryCache(
            max_size=self.config.memory_cache_size,
            ttl_seconds=self.config.memory_ttl_seconds
        )
        
        self.l2 = RedisCache(self.config)
        
        # MÃ©triques globales
        self.total_requests = 0
        self.l1_hits = 0
        self.l2_hits = 0
        self.l3_misses = 0
        
        logger.info("ğŸš€ Cache system initialized")
        logger.info(f"   L1 (Memory): {self.config.memory_cache_size} items, {self.config.memory_ttl_seconds}s TTL")
        logger.info(f"   L2 (Redis): {'ENABLED' if self.l2.is_available else 'DISABLED'}")
    
    def _generate_key(
        self, 
        model: str, 
        prompt: str, 
        options: Optional[Dict] = None
    ) -> str:
        """GÃ©nÃ¨re une clÃ© de cache"""
        return CacheKeyGenerator.generate(
            model, 
            prompt, 
            options, 
            self.config.redis_prefix
        )
    
    def get(
        self, 
        model: str, 
        prompt: str, 
        options: Optional[Dict] = None
    ) -> Optional[str]:
        """
        RÃ©cupÃ¨re du cache (L1 â†’ L2)
        
        Returns:
            Response cachÃ©e ou None
        """
        self.total_requests += 1
        
        # VÃ©rifier si on doit cacher
        if not CacheKeyGenerator.should_cache(prompt, self.config):
            return None
        
        key = self._generate_key(model, prompt, options)
        
        # L1: Memory cache
        value = self.l1.get(key)
        if value:
            self.l1_hits += 1
            logger.debug(f"âœ… L1 HIT: {key[:20]}...")
            return value
        
        # L2: Redis cache
        value = self.l2.get(key)
        if value:
            self.l2_hits += 1
            logger.debug(f"âœ… L2 HIT: {key[:20]}...")
            
            # Remonter en L1
            self.l1.set(key, value)
            return value
        
        # MISS complet
        self.l3_misses += 1
        logger.debug(f"âŒ MISS: {key[:20]}...")
        return None
    
    def set(
        self, 
        model: str, 
        prompt: str, 
        value: str,
        options: Optional[Dict] = None,
        ttl: Optional[int] = None
    ):
        """
        Stocke dans le cache (L1 + L2)
        
        Args:
            model: Nom du modÃ¨le
            prompt: Le prompt
            value: La rÃ©ponse Ã  cacher
            options: Options du modÃ¨le
            ttl: TTL custom (optionnel)
        """
        if not CacheKeyGenerator.should_cache(prompt, self.config):
            return
        
        key = self._generate_key(model, prompt, options)
        
        # L1: Memory cache
        self.l1.set(key, value)
        
        # L2: Redis cache
        self.l2.set(key, value, ttl)
        
        logger.debug(f"ğŸ’¾ CACHED: {key[:20]}... ({len(value)} chars)")
    
    def get_or_generate(
        self,
        model: str,
        prompt: str,
        generator: Callable[[], str],
        options: Optional[Dict] = None,
        ttl: Optional[int] = None
    ) -> str:
        """
        RÃ©cupÃ¨re du cache ou gÃ©nÃ¨re si absent
        
        Args:
            model: Nom du modÃ¨le
            prompt: Le prompt
            generator: Fonction pour gÃ©nÃ©rer la rÃ©ponse si absente
            options: Options du modÃ¨le
            ttl: TTL custom
        
        Returns:
            RÃ©ponse (cachÃ©e ou gÃ©nÃ©rÃ©e)
        """
        # Essayer le cache
        cached = self.get(model, prompt, options)
        if cached:
            return cached
        
        # GÃ©nÃ©rer
        start_time = time.time()
        result = generator()
        generation_time = time.time() - start_time
        
        logger.info(f"âš¡ Generated in {generation_time:.2f}s")
        
        # Cacher le rÃ©sultat
        self.set(model, prompt, result, options, ttl)
        
        return result
    
    def invalidate(self, model: Optional[str] = None):
        """
        Invalide le cache
        
        Args:
            model: Si spÃ©cifiÃ©, invalide seulement ce modÃ¨le
        """
        if model:
            pattern = f"{self.config.redis_prefix}:{model}:*"
            self.l2.clear(pattern)
            logger.info(f"ğŸ—‘ï¸  Cache invalidated for model: {model}")
        else:
            self.l1.clear()
            self.l2.clear()
            logger.info("ğŸ—‘ï¸  All cache invalidated")
    
    def get_stats(self) -> Dict:
        """Statistiques complÃ¨tes du cache"""
        total = self.total_requests
        
        if total == 0:
            overall_hit_rate = 0
        else:
            overall_hit_rate = ((self.l1_hits + self.l2_hits) / total) * 100
        
        return {
            "total_requests": total,
            "overall_hit_rate": f"{overall_hit_rate:.1f}%",
            "l1_hits": self.l1_hits,
            "l2_hits": self.l2_hits,
            "l3_misses": self.l3_misses,
            "l1_stats": self.l1.stats(),
            "l2_stats": self.l2.stats(),
            "speedup": f"{(self.l1_hits + self.l2_hits) / max(self.l3_misses, 1):.1f}x"
        }
    
    def print_stats(self):
        """Affiche les statistiques de maniÃ¨re lisible"""
        stats = self.get_stats()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š CACHE STATISTICS")
        print("=" * 60)
        print(f"Total Requests:     {stats['total_requests']}")
        print(f"Overall Hit Rate:   {stats['overall_hit_rate']}")
        print(f"Speedup:            {stats['speedup']}")
        print()
        print(f"L1 Hits (Memory):   {stats['l1_hits']}")
        print(f"L2 Hits (Redis):    {stats['l2_hits']}")
        print(f"L3 Misses (Gen):    {stats['l3_misses']}")
        print()
        print(f"L1 Memory Usage:    {stats['l1_stats']['usage']}")
        print(f"L2 Redis Status:    {'âœ… Available' if stats['l2_stats'].get('available') else 'âŒ Unavailable'}")
        print("=" * 60)

# ============================================================================
# EXEMPLE D'UTILISATION
# ============================================================================

if __name__ == "__main__":
    """DÃ©monstration du systÃ¨me de cache"""
    
    print("ğŸš€ Cache System Demo\n")
    
    # Configuration
    config = CacheConfig(
        memory_cache_size=100,
        memory_ttl_seconds=300,
        redis_enabled=True,
        redis_host="localhost",
        redis_port=6379
    )
    
    # Initialiser
    cache = CacheSystem(config)
    
    # Simuler des requÃªtes
    def fake_ollama_call(prompt: str) -> str:
        """Simule un appel Ollama (lent)"""
        time.sleep(0.1)  # Simule latence
        return f"Response to: {prompt}"
    
    print("\n1ï¸âƒ£  First request (should MISS and generate)...")
    result1 = cache.get_or_generate(
        model="qwen2.5:8b",
        prompt="What is Python?",
        generator=lambda: fake_ollama_call("What is Python?")
    )
    print(f"   Result: {result1[:50]}...")
    
    print("\n2ï¸âƒ£  Second request (should HIT from L1)...")
    result2 = cache.get_or_generate(
        model="qwen2.5:8b",
        prompt="What is Python?",
        generator=lambda: fake_ollama_call("What is Python?")
    )
    print(f"   Result: {result2[:50]}...")
    
    print("\n3ï¸âƒ£  Different request (should MISS)...")
    result3 = cache.get_or_generate(
        model="qwen2.5:8b",
        prompt="What is JavaScript?",
        generator=lambda: fake_ollama_call("What is JavaScript?")
    )
    print(f"   Result: {result3[:50]}...")
    
    # Afficher les stats
    cache.print_stats()
    
    print("\nâœ… Demo complete!")
