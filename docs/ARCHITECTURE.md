# Architecture Technique - LLMUI Core v0.5

**Francois Chalut**

---

## üéØ Vue d'ensemble

LLMUI Core est une plateforme de consensus multi-mod√®les bas√©e sur une architecture microservices avec orchestration de LLM locaux via Ollama.

### Principe fondamental

Au lieu d'utiliser un seul mod√®le LLM, LLMUI Core utilise **plusieurs mod√®les en parall√®le** (workers) qui analysent le prompt, puis un **mod√®le merger** qui synth√©tise leurs r√©ponses pour obtenir un consensus de qualit√© sup√©rieure.

```
Prompt ‚Üí Workers (phi3, gemma2) ‚Üí Analyses ‚Üí Merger (granite4) ‚Üí Consensus ‚Üí R√©ponse
```

---

## üèóÔ∏è Architecture globale

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         UTILISATEUR                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NGINX (Port 80/443)                           ‚îÇ
‚îÇ  ‚Ä¢ Reverse proxy                                                 ‚îÇ
‚îÇ  ‚Ä¢ SSL/TLS termination                                           ‚îÇ
‚îÇ  ‚Ä¢ Serveur de fichiers statiques (web/)                         ‚îÇ
‚îÇ  ‚Ä¢ Headers de s√©curit√©                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  LLMUI PROXY (Port 8080)                         ‚îÇ
‚îÇ  ‚Ä¢ Authentification JWT                                          ‚îÇ
‚îÇ  ‚Ä¢ Gestion des sessions                                          ‚îÇ
‚îÇ  ‚Ä¢ Rate limiting                                                 ‚îÇ
‚îÇ  ‚Ä¢ Validation des requ√™tes                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 LLMUI BACKEND (Port 5000)                        ‚îÇ
‚îÇ  ‚Ä¢ API REST FastAPI                                              ‚îÇ
‚îÇ  ‚Ä¢ Orchestration LLM                                             ‚îÇ
‚îÇ  ‚Ä¢ Syst√®me de consensus                                          ‚îÇ
‚îÇ  ‚Ä¢ Gestion m√©moire (court/long terme, RAG)                      ‚îÇ
‚îÇ  ‚Ä¢ Traitement de fichiers                                        ‚îÇ
‚îÇ  ‚Ä¢ Base de donn√©es SQLite                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   OLLAMA (Port 11434)                            ‚îÇ
‚îÇ  ‚Ä¢ Serveur LLM local                                             ‚îÇ
‚îÇ  ‚Ä¢ Workers: phi3:3.8b, gemma2:2b                                ‚îÇ
‚îÇ  ‚Ä¢ Merger: granite4:micro-h                                     ‚îÇ
‚îÇ  ‚Ä¢ Gestion GPU/CPU                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Composants d√©taill√©s

### 1. Frontend (web/)

**Technologie**: HTML5, CSS3, JavaScript vanilla

**Structure**:
```
web/
‚îú‚îÄ‚îÄ index.html              # Page principale (chat)
‚îú‚îÄ‚îÄ login.html             # Authentification
‚îú‚îÄ‚îÄ installer.html         # Interface installation (optionnel)
‚îú‚îÄ‚îÄ *.css                  # Styles modulaires
‚îî‚îÄ‚îÄ *.js                   # Modules JavaScript
```

**Modules JavaScript**:
- `app.js` - Application principale
- `auth.js` - Gestion authentification
- `ui.js` - Interface utilisateur
- `utils.js` - Utilitaires
- `message_editor.js` - √âdition messages
- `dialog-system.js` - Dialogues modaux
- `session-guard.js` - Protection session
- `i18n.js` - Internationalisation
- `fallback.js` - Gestion erreurs

**Fonctionnalit√©s**:
- Chat en temps r√©el
- Upload de fichiers (drag & drop)
- √âdition de messages
- Historique conversations
- Mode sombre/clair
- Responsive (mobile/desktop)

### 2. Proxy (llmui_proxy.py)

**Technologie**: Python 3.8+, aiohttp

**R√¥le**: Couche de s√©curit√© et gestion des sessions

**Fonctionnalit√©s**:
```python
# Authentification
- G√©n√©ration JWT tokens
- Validation tokens
- Refresh tokens
- Logout/session cleanup

# S√©curit√©
- Rate limiting (SlowAPI)
- Validation inputs
- Protection CSRF
- Headers de s√©curit√©

# Sessions
- Stockage session (SQLite)
- Timeout automatique
- Multi-sessions par utilisateur
```

**Endpoints**:
- `POST /api/auth/login` - Connexion
- `POST /api/auth/logout` - D√©connexion
- `POST /api/auth/refresh` - Refresh token
- `GET /api/auth/verify` - V√©rification token

**Base de donn√©es**:
```sql
sessions
‚îú‚îÄ‚îÄ session_id (PK)
‚îú‚îÄ‚îÄ user_id
‚îú‚îÄ‚îÄ token
‚îú‚îÄ‚îÄ created_at
‚îú‚îÄ‚îÄ expires_at
‚îî‚îÄ‚îÄ last_activity
```

### 3. Backend (llmui_backend.py)

**Technologie**: FastAPI, Uvicorn, aiosqlite

**Architecture interne**:
```python
llmui_backend.py
‚îú‚îÄ‚îÄ FastAPI app
‚îú‚îÄ‚îÄ Ollama client
‚îú‚îÄ‚îÄ ConsensusEngine
‚îú‚îÄ‚îÄ MemoryManager
‚îú‚îÄ‚îÄ FileProcessor
‚îú‚îÄ‚îÄ DatabaseManager
‚îî‚îÄ‚îÄ CachingSystem
```

#### 3.1 API REST

**Endpoints principaux**:

```python
# Health & Info
GET  /api/health          # Statut syst√®me
GET  /api/models          # Liste mod√®les Ollama
GET  /api/stats           # Statistiques utilisation

# Chat
POST /api/chat            # Nouvelle conversation
GET  /api/conversations   # Historique
GET  /api/conversation/{id} # D√©tails conversation
DELETE /api/conversation/{id} # Supprimer

# Fichiers
POST /api/upload          # Upload fichier
POST /api/process_file    # Traiter fichier
GET  /api/files           # Liste fichiers

# Utilisateurs
GET  /api/user/profile    # Profil utilisateur
PUT  /api/user/profile    # Mise √† jour profil
GET  /api/user/settings   # Param√®tres

# Configuration
GET  /api/config          # Configuration syst√®me
PUT  /api/config          # Mise √† jour config
```

#### 3.2 Syst√®me de consensus

**Algorithme**:

```python
def consensus_chat(prompt: str) -> str:
    # 1. Phase Workers - Analyse parall√®le
    worker_responses = []
    for model in ['phi3:3.8b', 'gemma2:2b']:
        response = ollama.generate(
            model=model,
            prompt=f"Analysez: {prompt}"
        )
        worker_responses.append(response)
    
    # 2. Phase Merger - Synth√®se
    merger_prompt = f"""
    Voici diff√©rentes analyses d'un prompt:
    
    Analyse 1 (phi3): {worker_responses[0]}
    Analyse 2 (gemma2): {worker_responses[1]}
    
    Synth√©tisez la meilleure r√©ponse en tenant compte de toutes les analyses.
    """
    
    consensus = ollama.generate(
        model='granite4:micro-h',
        prompt=merger_prompt
    )
    
    return consensus
```

**Strat√©gies de consensus**:
- `majority` - Vote majoritaire simple
- `weighted` - Vote pond√©r√© par confiance
- `merger` - Synth√®se par mod√®le d√©di√© (d√©faut)
- `best_of` - Meilleure r√©ponse selon scoring

#### 3.3 Syst√®me de m√©moire

**Types de m√©moire**:

```python
class MemoryManager:
    def __init__(self):
        self.short_term = {}      # Contexte conversation
        self.long_term = SQLite   # Historique persistant
        self.rag = VectorDB       # Base vectorielle
        self.cache = LRUCache     # Cache r√©ponses
```

**M√©moire court terme**:
```python
{
    "conversation_id": "conv_123",
    "messages": [
        {"role": "user", "content": "..."},
        {"role": "assistant", "content": "..."}
    ],
    "context": {
        "user_preferences": {...},
        "conversation_metadata": {...}
    }
}
```

**M√©moire long terme (SQLite)**:
```sql
conversations
‚îú‚îÄ‚îÄ id (PK)
‚îú‚îÄ‚îÄ user_id (FK)
‚îú‚îÄ‚îÄ title
‚îú‚îÄ‚îÄ created_at
‚îú‚îÄ‚îÄ updated_at
‚îî‚îÄ‚îÄ metadata (JSON)

messages
‚îú‚îÄ‚îÄ id (PK)
‚îú‚îÄ‚îÄ conversation_id (FK)
‚îú‚îÄ‚îÄ role (user/assistant/system)
‚îú‚îÄ‚îÄ content (TEXT)
‚îú‚îÄ‚îÄ tokens_used
‚îî‚îÄ‚îÄ created_at

embeddings (RAG)
‚îú‚îÄ‚îÄ id (PK)
‚îú‚îÄ‚îÄ content_id
‚îú‚îÄ‚îÄ embedding (BLOB)
‚îî‚îÄ‚îÄ metadata (JSON)
```

**RAG (Retrieval-Augmented Generation)**:
```python
# 1. Indexation documents
def index_document(doc: str):
    chunks = split_into_chunks(doc)
    for chunk in chunks:
        embedding = generate_embedding(chunk)
        store_embedding(chunk, embedding)

# 2. Recherche s√©mantique
def semantic_search(query: str, top_k=5):
    query_embedding = generate_embedding(query)
    similar_chunks = find_similar(query_embedding, top_k)
    return similar_chunks

# 3. Augmentation du prompt
def augmented_prompt(user_query: str):
    relevant_context = semantic_search(user_query)
    return f"""
    Contexte pertinent:
    {relevant_context}
    
    Question: {user_query}
    """
```

#### 3.4 Traitement de fichiers

**Formats support√©s**:
- **Documents**: PDF, DOCX, TXT, MD, RTF
- **Images**: PNG, JPG, JPEG, WEBP, GIF
- **Donn√©es**: CSV, JSON, YAML, XML
- **Code**: Python, JavaScript, etc.

**Pipeline de traitement**:
```python
def process_file(file: UploadFile):
    # 1. Validation
    validate_file_type(file)
    validate_file_size(file)
    
    # 2. Extraction contenu
    if file.type == 'pdf':
        content = extract_pdf(file)
    elif file.type == 'docx':
        content = extract_docx(file)
    elif file.type == 'image':
        content = ocr_image(file)
    
    # 3. Indexation (optionnel)
    if should_index(file):
        index_document(content)
    
    # 4. Analyse
    summary = llm_summarize(content)
    
    return {
        "content": content,
        "summary": summary,
        "metadata": extract_metadata(file)
    }
```

#### 3.5 Cache syst√®me

**Strat√©gie de cache**:
```python
class CachingSystem:
    def __init__(self):
        self.response_cache = LRUCache(maxsize=1000)
        self.embedding_cache = LRUCache(maxsize=5000)
        
    def cache_key(self, prompt: str, params: dict) -> str:
        return hashlib.sha256(
            f"{prompt}:{json.dumps(params)}"
        ).hexdigest()
    
    def get_cached_response(self, prompt: str, params: dict):
        key = self.cache_key(prompt, params)
        return self.response_cache.get(key)
    
    def cache_response(self, prompt: str, params: dict, response: str):
        key = self.cache_key(prompt, params)
        self.response_cache[key] = response
```

### 4. Ollama

**Configuration**:
```yaml
ollama:
  base_url: "http://localhost:11434"
  timeout: 300
  
  models:
    workers:
      - name: "phi3:3.8b"
        role: "worker"
        temperature: 0.7
        
      - name: "gemma2:2b"
        role: "worker"
        temperature: 0.8
        
    merger:
      name: "granite4:micro-h"
      role: "merger"
      temperature: 0.5
```

**Gestion des mod√®les**:
```python
class OllamaManager:
    def list_models(self):
        """Liste mod√®les disponibles"""
        
    def pull_model(self, model: str):
        """T√©l√©charge un mod√®le"""
        
    def remove_model(self, model: str):
        """Supprime un mod√®le"""
        
    def model_info(self, model: str):
        """Infos sur un mod√®le"""
```

---

## üîÑ Flux de donn√©es d√©taill√©s

### Flux 1: Conversation simple

```
1. User envoie message
   ‚Üì
2. Frontend ‚Üí POST /api/chat
   ‚Üì
3. Nginx ‚Üí route vers Proxy:8080
   ‚Üì
4. Proxy v√©rifie JWT token
   ‚Üì
5. Proxy ‚Üí forward vers Backend:5000
   ‚Üì
6. Backend r√©cup√®re contexte (m√©moire)
   ‚Üì
7. Backend ‚Üí Ollama workers (parall√®le)
   ‚îú‚îÄ‚Üí phi3:3.8b analyse
   ‚îî‚îÄ‚Üí gemma2:2b analyse
   ‚Üì
8. Backend re√ßoit analyses workers
   ‚Üì
9. Backend ‚Üí Ollama merger (granite4)
   ‚Üì
10. Backend re√ßoit consensus
    ‚Üì
11. Backend stocke dans DB
    ‚Üì
12. Backend ‚Üí retourne r√©ponse
    ‚Üì
13. Proxy ‚Üí forward r√©ponse
    ‚Üì
14. Nginx ‚Üí retourne au Frontend
    ‚Üì
15. Frontend affiche r√©ponse
```

### Flux 2: Upload et analyse de fichier

```
1. User upload fichier
   ‚Üì
2. Frontend ‚Üí POST /api/upload (multipart)
   ‚Üì
3. Nginx ‚Üí Proxy ‚Üí Backend
   ‚Üì
4. Backend valide fichier
   ‚Üì
5. Backend extrait contenu
   ‚îú‚îÄ‚Üí PDF: PyPDF2
   ‚îú‚îÄ‚Üí DOCX: python-docx
   ‚îú‚îÄ‚Üí Image: OCR (tesseract)
   ‚îî‚îÄ‚Üí Texte: direct
   ‚Üì
6. Backend indexe (RAG optionnel)
   ‚Üì
7. Backend ‚Üí Ollama pour r√©sum√©
   ‚Üì
8. Backend stocke m√©tadonn√©es
   ‚Üì
9. Backend retourne r√©sum√© + ID
   ‚Üì
10. User peut poser questions sur fichier
```

### Flux 3: WebSocket (streaming)

```
1. Frontend ouvre WebSocket
   ‚Üì
2. ws://localhost/ws/chat
   ‚Üì
3. Nginx upgrade to WebSocket
   ‚Üì
4. Backend accepte connexion
   ‚Üì
5. User envoie message
   ‚Üì
6. Backend ‚Üí Ollama (streaming mode)
   ‚Üì
7. Backend re√ßoit tokens progressifs
   ‚Üì
8. Backend ‚Üí envoie chaque token via WS
   ‚Üì
9. Frontend affiche en temps r√©el
   ‚Üì
10. Fin de g√©n√©ration ‚Üí fermeture ou maintien
```

---

## üóÑÔ∏è Sch√©ma de base de donn√©es

### Tables principales

```sql
-- Utilisateurs
CREATE TABLE users (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username TEXT UNIQUE NOT NULL,
    email TEXT UNIQUE,
    password_hash TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP,
    preferences JSON
);

-- Conversations
CREATE TABLE conversations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    title TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSON,
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- Messages
CREATE TABLE messages (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    conversation_id INTEGER NOT NULL,
    role TEXT NOT NULL CHECK(role IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    tokens_used INTEGER,
    model_used TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id)
);

-- Sessions
CREATE TABLE sessions (
    id TEXT PRIMARY KEY,
    user_id INTEGER NOT NULL,
    token TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP NOT NULL,
    last_activity TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- Fichiers upload√©s
CREATE TABLE uploaded_files (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    filename TEXT NOT NULL,
    file_type TEXT,
    file_size INTEGER,
    storage_path TEXT,
    summary TEXT,
    uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSON,
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- Cache r√©ponses
CREATE TABLE response_cache (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    prompt_hash TEXT UNIQUE NOT NULL,
    prompt TEXT,
    response TEXT,
    model_used TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    hits INTEGER DEFAULT 0
);

-- Embeddings pour RAG
CREATE TABLE embeddings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    content_id TEXT,
    content TEXT,
    embedding BLOB,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## üîê S√©curit√©

### Couches de s√©curit√©

1. **R√©seau**
   - Pare-feu (UFW/firewalld)
   - Ports ferm√©s sauf 22, 80, 443
   - Fail2ban pour brute-force SSH

2. **Transport**
   - HTTPS/TLS (Let's Encrypt)
   - Headers de s√©curit√© Nginx
   - HSTS activ√©

3. **Authentification**
   - JWT tokens avec expiration
   - Bcrypt pour passwords (12 rounds)
   - Refresh tokens
   - Session timeout

4. **Application**
   - Validation inputs (Pydantic)
   - Rate limiting (100 req/min)
   - CORS configur√©
   - Protection CSRF

5. **Syst√®me**
   - Services systemd isol√©s
   - Permissions strictes (700 data/)
   - Utilisateur d√©di√© (llmui)
   - Pas de root access

### Headers de s√©curit√© Nginx

```nginx
add_header X-Frame-Options "SAMEORIGIN" always;
add_header X-Content-Type-Options "nosniff" always;
add_header X-XSS-Protection "1; mode=block" always;
add_header Referrer-Policy "strict-origin-when-cross-origin" always;
add_header Content-Security-Policy "default-src 'self'" always;
```

---

## ‚ö° Performance

### Optimisations

1. **Cache multi-niveaux**
   - R√©ponses LLM (1000 entr√©es)
   - Embeddings (5000 entr√©es)
   - Sessions (m√©moire)

2. **Traitement asynchrone**
   - AsyncIO pour I/O
   - Workers Ollama en parall√®le
   - WebSocket pour streaming

3. **Base de donn√©es**
   - Index sur colonnes fr√©quentes
   - Nettoyage automatique vieux messages
   - Vacuum p√©riodique SQLite

4. **R√©seau**
   - Compression gzip Nginx
   - CDN pour assets statiques (optionnel)
   - Keep-alive connexions

### M√©triques typiques

- Temps r√©ponse API: < 100ms (sans LLM)
- G√©n√©ration LLM: 2-10 secondes
- Consensus complet: 5-15 secondes
- M√©moire utilis√©e: 500MB-2GB
- CPU: 2-4 c≈ìurs recommand√©s

---

## üìä Monitoring

### Logs

```bash
# Application
/opt/llmui-core/logs/llmui.log

# Services
journalctl -u llmui-backend
journalctl -u llmui-proxy

# Nginx
/var/log/nginx/llmui-access.log
/var/log/nginx/llmui-error.log

# Ollama
journalctl -u ollama
```

### M√©triques expos√©es

```python
GET /api/stats
{
    "requests_total": 1523,
    "requests_per_minute": 12.5,
    "avg_response_time": 3.2,
    "cache_hit_rate": 0.45,
    "active_conversations": 8,
    "models_loaded": ["phi3:3.8b", "gemma2:2b", "granite4:micro-h"]
}
```

---

## üîß Configuration

Voir `config.yaml.example` et `docs/CONFIGURATION.md` pour d√©tails.

---


**Francois Chalut**  
*Architecture pour la souverainet√© num√©rique* üá®üá¶
