# üöÄ D√©marrage rapide - LLMUI Core

Guide ultra-rapide pour installer et utiliser LLMUI Core en moins de 15 minutes.

---

## ‚ö° Installation express

### Pr√©requis
- Serveur Linux (Debian/Ubuntu/Rocky/RHEL)
- Python 3.8+
- Acc√®s root (sudo)
- 8GB RAM minimum
- 20GB disque libre

### 3 commandes pour tout installer

```bash
# 1. Cloner le d√©p√¥t
git clone https://github.com/votre-repo/llmui-core.git && cd llmui-core

# 2. Lancer l'installation interactive
sudo bash andy_setup.sh

# 3. Choisir option [1] - Installation compl√®te
```

**C'est tout!** Andy s'occupe du reste. ‚òï

---

## üìã Ce qui va se passer

Andy va automatiquement:

1. ‚úÖ Mettre √† jour votre syst√®me
2. ‚úÖ Installer Ollama + 3 mod√®les LLM (~10 minutes)
3. ‚úÖ Configurer Python et d√©pendances
4. ‚úÖ Cr√©er les services systemd
5. ‚úÖ Configurer Nginx
6. ‚úÖ Configurer le pare-feu
7. ‚úÖ D√©marrer tous les services
8. ‚úÖ Afficher l'URL d'acc√®s

**Dur√©e totale**: 15-30 minutes (selon connexion internet)

---

## üéØ Premi√®re utilisation

### Acc√©der √† l'interface

Une fois l'installation termin√©e, Andy affiche:

```
‚úì Interface accessible sur: http://192.168.1.100/
```

Ouvrez cette URL dans votre navigateur!

### Premiers pas

1. **Page d'accueil**
   - Interface de chat moderne
   - Historique des conversations
   - Upload de fichiers

2. **Premi√®re conversation**
   ```
   Vous: Bonjour! Peux-tu m'expliquer comment tu fonctionnes?
   
   LLMUI: Bonjour! Je suis LLMUI Core, une plateforme de consensus...
   ```

3. **Upload de fichier**
   - Cliquez sur üìé
   - S√©lectionnez un PDF, DOCX ou image
   - Posez des questions sur le document

---

## üõ†Ô∏è Commandes utiles

### V√©rifier l'√©tat

```bash
# Via Andy
sudo bash andy_setup.sh
# Choisir option [5] - V√©rifier l'installation

# Ou manuellement
sudo systemctl status llmui-backend llmui-proxy nginx
```

### Consulter les logs

```bash
# Backend
sudo journalctl -u llmui-backend -f

# Proxy
sudo journalctl -u llmui-proxy -f

# Nginx
sudo tail -f /var/log/nginx/llmui-access.log
```

### Red√©marrer les services

```bash
sudo systemctl restart llmui-backend llmui-proxy nginx
```

---

## üì± API REST rapide

### Health check

```bash
curl http://localhost:5000/api/health
```

### Liste des mod√®les

```bash
curl http://localhost:5000/api/models
```

### Envoyer un message

```bash
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Bonjour!",
    "user_id": "test_user"
  }'
```

---

## üîß Configuration rapide

### Fichier principal

√âditez `/opt/llmui-core/config.yaml`:

```yaml
server:
  port: 5000
  
ollama:
  models:
    workers:
      - "phi3:3.8b"
      - "gemma2:2b"
    merger: "granite4:micro-h"

# Red√©marrer apr√®s modification
```

```bash
sudo systemctl restart llmui-backend llmui-proxy
```

### Ajouter SSL (optionnel)

```bash
sudo certbot --nginx -d votre-domaine.com
```

---

## üêõ Probl√®mes courants

### Les services ne d√©marrent pas

```bash
# V√©rifier les logs
sudo journalctl -u llmui-backend -n 50

# V√©rifier les permissions
ls -la /opt/llmui-core/

# Red√©marrer
sudo systemctl restart llmui-backend llmui-proxy
```

### Page blanche dans le navigateur

```bash
# V√©rifier Nginx
sudo systemctl status nginx
sudo nginx -t

# V√©rifier le backend
curl http://localhost:5000/api/health
```

### Ollama ne r√©pond pas

```bash
# V√©rifier Ollama
ollama list
ollama ps

# Red√©marrer
sudo systemctl restart ollama

# Tester un mod√®le
ollama run phi3:3.8b "test"
```

---

## üìö Aller plus loin

### Documentation compl√®te

- **[README.md](README.md)** - Vue d'ensemble compl√®te
- **[INSTALL.md](INSTALL.md)** - Guide d'installation d√©taill√©
- **[README_ANDY.md](README_ANDY.md)** - Documentation Andy
- **[ANDY_INTERACTIVE.md](ANDY_INTERACTIVE.md)** - Guide menu interactif
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Contribuer au projet

### Tutoriels

1. **Configuration avanc√©e**: INSTALL.md section "Configuration post-installation"
2. **API REST compl√®te**: docs/API.md
3. **Architecture technique**: docs/ARCHITECTURE.md
4. **D√©pannage**: docs/TROUBLESHOOTING.md

### Support

- **GitHub Issues**: [Cr√©er un issue](https://github.com/votre-repo/llmui-core/issues)
- **Email**: support@genie-ia.ca
- **Documentation**: [Wiki](https://github.com/votre-repo/llmui-core/wiki)

---

## üéì Exemples d'utilisation

### Chat simple

```python
import requests

response = requests.post('http://localhost:5000/api/chat', json={
    "message": "Explique-moi l'intelligence artificielle",
    "user_id": "user123"
})

print(response.json()['response'])
```

### Upload et analyse de document

```python
files = {'file': open('document.pdf', 'rb')}
data = {
    'user_id': 'user123',
    'question': 'R√©sume ce document'
}

response = requests.post(
    'http://localhost:5000/api/upload',
    files=files,
    data=data
)

print(response.json()['summary'])
```

### Conversation avec contexte

```python
# Premier message
response1 = requests.post('http://localhost:5000/api/chat', json={
    "message": "Mon pr√©nom est Fran√ßois",
    "user_id": "user123",
    "conversation_id": "conv_001"
})

# Message avec contexte
response2 = requests.post('http://localhost:5000/api/chat', json={
    "message": "Quel est mon pr√©nom?",
    "user_id": "user123",
    "conversation_id": "conv_001"
})

print(response2.json()['response'])  # "Fran√ßois"
```

---

## ‚öôÔ∏è Configuration par cas d'usage

### Serveur de d√©veloppement

```yaml
# config.yaml
server:
  debug: true
  
logging:
  level: "DEBUG"
```

### Production

```yaml
# config.yaml
server:
  ssl_enabled: true
  ssl_cert: "/opt/llmui-core/ssl/cert.pem"
  ssl_key: "/opt/llmui-core/ssl/key.pem"
  
security:
  max_login_attempts: 5
  lockout_duration: 900
  
logging:
  level: "INFO"
```

### Performance maximale

```yaml
# config.yaml
ollama:
  timeout: 120  # R√©duire pour r√©ponses plus rapides
  
memory:
  type: "short_term"  # D√©sactiver RAG pour vitesse
  max_tokens: 2048
```

---

## üö¶ Checklist post-installation

- [ ] Services d√©marr√©s (backend, proxy, nginx)
- [ ] Interface accessible via navigateur
- [ ] Test API health check OK
- [ ] Test envoi de message OK
- [ ] Ollama r√©pond correctement
- [ ] Logs sans erreurs
- [ ] Pare-feu configur√©
- [ ] SSL configur√© (si production)
- [ ] Backup configur√©

---

## üéâ F√©licitations!

Vous avez maintenant une installation compl√®te de LLMUI Core!

### Prochaines √©tapes sugg√©r√©es

1. ‚úÖ Tester diff√©rents types de questions
2. ‚úÖ Uploader et analyser des documents
3. ‚úÖ Explorer l'API REST
4. ‚úÖ Configurer SSL pour la production
5. ‚úÖ Lire la documentation compl√®te
6. ‚úÖ Rejoindre la communaut√©

---

## üí° Conseils pro

### Performance

- Utilisez un SSD pour `/opt/llmui-core/data`
- Minimum 8GB RAM, 16GB recommand√©
- GPU optionnel mais am√©liore beaucoup les performances

### S√©curit√©

- Changez le mot de passe par d√©faut
- Activez SSL en production
- Mettez √† jour r√©guli√®rement
- Consultez les logs quotidiennement

### Maintenance

```bash
# Backup quotidien
sudo tar -czf ~/llmui-backup-$(date +%Y%m%d).tar.gz /opt/llmui-core/data

# Nettoyage des logs anciens
sudo journalctl --vacuum-time=7d

# Mise √† jour
cd /path/to/llmui-core && git pull
sudo python3 andy_deploy_source.py
sudo systemctl restart llmui-backend llmui-proxy
```

---

## üìû Besoin d'aide?

### Ressources

- üìñ **Documentation**: README.md, INSTALL.md
- üêõ **Bugs**: [GitHub Issues](https://github.com/votre-repo/llmui-core/issues)
- üí¨ **Discussions**: [GitHub Discussions](https://github.com/votre-repo/llmui-core/discussions)
- üìß **Email**: support@genie-ia.ca

### Avant de demander de l'aide

1. Consultez les logs: `sudo journalctl -u llmui-backend -n 100`
2. V√©rifiez la documentation: README.md et INSTALL.md
3. Recherchez dans les issues existantes
4. Pr√©parez les informations syst√®me:
   ```bash
   uname -a
   python3 --version
   ollama --version
   systemctl status llmui-backend llmui-proxy nginx
   ```

---

**üéä Bienvenue dans l'√©cosyst√®me LLMUI Core!**

*Pour la souverainet√© num√©rique du Qu√©bec* üá®üá¶

---

**G√©nie IA Centre Op√©rationnel S√©curit√© inc.** - 2025
