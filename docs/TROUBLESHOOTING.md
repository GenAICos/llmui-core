# Guide de d√©pannage - LLMUI Core v0.5.0

Guide complet pour r√©soudre les probl√®mes courants de LLMUI Core.

---

## üìã Table des mati√®res

1. [Diagnostic rapide](#diagnostic-rapide)
2. [Probl√®mes d'installation](#probl√®mes-dinstallation)
3. [Services ne d√©marrent pas](#services-ne-d√©marrent-pas)
4. [Erreurs de connexion](#erreurs-de-connexion)
5. [Probl√®mes Ollama](#probl√®mes-ollama)
6. [Erreurs de base de donn√©es](#erreurs-de-base-de-donn√©es)
7. [Probl√®mes de performance](#probl√®mes-de-performance)
8. [Probl√®mes r√©seau](#probl√®mes-r√©seau)
9. [Erreurs frontend](#erreurs-frontend)
10. [Logs et monitoring](#logs-et-monitoring)

---

## üîç Diagnostic rapide

### Commande de diagnostic complet

```bash
#!/bin/bash
# diagnostic_llmui.sh

echo "=== DIAGNOSTIC LLMUI CORE ==="
echo ""

echo "1. Services systemd:"
systemctl is-active llmui-backend llmui-proxy nginx ollama

echo ""
echo "2. Ports en √©coute:"
ss -tlnp | grep -E '(5000|8080|80|11434)'

echo ""
echo "3. Processus:"
ps aux | grep -E '(llmui|ollama|nginx)' | grep -v grep

echo ""
echo "4. Espace disque:"
df -h /opt/llmui-core

echo ""
echo "5. M√©moire:"
free -h

echo ""
echo "6. Test HTTP:"
curl -I http://localhost/

echo ""
echo "7. Test API:"
curl http://localhost:5000/api/health

echo ""
echo "8. Mod√®les Ollama:"
ollama list

echo ""
echo "9. Derni√®res erreurs backend:"
journalctl -u llmui-backend -n 10 --no-pager

echo ""
echo "10. Derni√®res erreurs Nginx:"
tail -5 /var/log/nginx/llmui-error.log
```

### Checklist rapide

- [ ] Services actifs: `sudo systemctl status llmui-backend llmui-proxy nginx`
- [ ] Ports ouverts: `sudo ss -tlnp | grep -E '(5000|8080|80)'`
- [ ] Ollama fonctionne: `ollama list`
- [ ] Base de donn√©es accessible: `ls -la /opt/llmui-core/data/*.db`
- [ ] Permissions correctes: `ls -la /opt/llmui-core/`
- [ ] Espace disque: `df -h /opt/llmui-core`

---

## üõ†Ô∏è Probl√®mes d'installation

### Andy √©choue pendant l'installation

**Sympt√¥me**: Le script `andy_installer.py` s'arr√™te avec une erreur.

**Solution**:
```bash
# 1. Consulter les logs Andy
less /tmp/andy_install.log

# 2. Consulter la base de donn√©es
sqlite3 /tmp/andy_installation.db
SELECT * FROM commands WHERE status='failed' ORDER BY timestamp DESC LIMIT 5;
.quit

# 3. Identifier l'√©tape probl√©matique et relancer
sudo python3 andy_installer.py
```

**Causes fr√©quentes**:
- **Pas de connexion internet**: V√©rifier `ping google.com`
- **Espace disque insuffisant**: `df -h`
- **Permissions**: V√©rifier que vous √™tes root/sudo
- **D√©p√¥t d√©j√† occup√©**: `sudo killall apt apt-get` (Debian/Ubuntu)

### Erreur "Package not found"

**Sympt√¥me**: `E: Unable to locate package python3-venv`

**Solution**:
```bash
# Debian/Ubuntu
sudo apt update
sudo apt-cache search python3-venv

# Si toujours pas trouv√©
sudo apt install software-properties-common
sudo add-apt-repository universe
sudo apt update
```

### Ollama ne s'installe pas

**Sympt√¥me**: `curl: command not found` ou erreur r√©seau

**Solution**:
```bash
# Installer curl
sudo apt install curl  # Debian/Ubuntu
sudo dnf install curl  # RHEL/Rocky

# Installation manuelle Ollama
wget https://ollama.com/download/ollama-linux-amd64
sudo mv ollama-linux-amd64 /usr/local/bin/ollama
sudo chmod +x /usr/local/bin/ollama

# Cr√©er le service
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama
sudo systemctl enable --now ollama
```

### Mod√®les LLM ne se t√©l√©chargent pas

**Sympt√¥me**: `ollama pull phi3:3.8b` √©choue

**Solution**:
```bash
# V√©rifier Ollama
sudo systemctl status ollama

# V√©rifier connexion
curl http://localhost:11434/api/tags

# Espace disque (mod√®les = ~6GB total)
df -h /usr/share/ollama

# T√©l√©charger manuellement avec verbose
ollama pull phi3:3.8b --verbose

# Si probl√®me r√©seau persistant
export OLLAMA_DEBUG=1
ollama pull phi3:3.8b
```

---

## üö´ Services ne d√©marrent pas

### Backend ne d√©marre pas

**Sympt√¥me**: `sudo systemctl start llmui-backend` √©choue

**Diagnostic**:
```bash
# Voir les erreurs
sudo journalctl -u llmui-backend -n 50

# V√©rifier le fichier service
cat /etc/systemd/system/llmui-backend.service

# Tester manuellement
sudo su - llmui
cd /opt/llmui-core
/opt/llmui-core/venv/bin/python src/llmui_backend.py
```

**Causes fr√©quentes**:

#### 1. Erreur Python/Import
```
ModuleNotFoundError: No module named 'fastapi'
```

**Solution**:
```bash
# R√©installer les d√©pendances
sudo su - llmui -c "/opt/llmui-core/venv/bin/pip install -r /opt/llmui-core/requirements.txt"
```

#### 2. Port d√©j√† utilis√©
```
OSError: [Errno 98] Address already in use
```

**Solution**:
```bash
# Identifier le processus
sudo lsof -i :5000

# Tuer le processus
sudo kill -9 <PID>

# Ou changer le port dans config.yaml
sudo nano /opt/llmui-core/config.yaml
# server:
#   port: 5001
```

#### 3. Permissions
```
PermissionError: [Errno 13] Permission denied: '/opt/llmui-core/data'
```

**Solution**:
```bash
# Corriger les permissions
sudo chown -R llmui:llmui /opt/llmui-core
sudo chmod -R 755 /opt/llmui-core
sudo chmod -R 700 /opt/llmui-core/data /opt/llmui-core/logs
```

#### 4. Configuration invalide
```
yaml.scanner.ScannerError: mapping values are not allowed here
```

**Solution**:
```bash
# V√©rifier la syntaxe YAML
python3 -c "import yaml; yaml.safe_load(open('/opt/llmui-core/config.yaml'))"

# Restaurer la config par d√©faut
sudo cp /opt/llmui-core/config.yaml.example /opt/llmui-core/config.yaml
sudo chown llmui:llmui /opt/llmui-core/config.yaml
```

### Proxy ne d√©marre pas

**Sympt√¥me**: `sudo systemctl start llmui-proxy` √©choue

**Solution**:
```bash
# Logs
sudo journalctl -u llmui-proxy -n 50

# V√©rifier que le backend est actif
sudo systemctl status llmui-backend

# Port 8080 libre?
sudo lsof -i :8080

# Test manuel
sudo su - llmui -c "/opt/llmui-core/venv/bin/python /opt/llmui-core/src/llmui_proxy.py"
```

### Nginx ne d√©marre pas

**Sympt√¥me**: `sudo systemctl start nginx` √©choue

**Diagnostic**:
```bash
# Test de configuration
sudo nginx -t

# Logs
sudo journalctl -u nginx -n 50
sudo tail -50 /var/log/nginx/error.log

# V√©rifier les permissions
ls -la /opt/llmui-core/web/
```

**Causes fr√©quentes**:

#### 1. Erreur de syntaxe
```
nginx: [emerg] unexpected "}" in /etc/nginx/sites-enabled/llmui:42
```

**Solution**:
```bash
# √âditer la config
sudo nano /etc/nginx/sites-available/llmui

# Ou restaurer depuis backup
sudo cp /etc/nginx/sites-available/llmui.bak.* /etc/nginx/sites-available/llmui
```

#### 2. Port 80 d√©j√† utilis√©
```
nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)
```

**Solution**:
```bash
# Identifier le processus
sudo lsof -i :80

# Apache en conflit?
sudo systemctl stop apache2
sudo systemctl disable apache2
```

---

## üîå Erreurs de connexion

### Erreur 502 Bad Gateway

**Sympt√¥me**: Page web affiche "502 Bad Gateway"

**Cause**: Backend ou proxy ne r√©pond pas

**Solution**:
```bash
# 1. V√©rifier les services
sudo systemctl status llmui-backend llmui-proxy

# 2. D√©marrer si arr√™t√©s
sudo systemctl start llmui-backend
sleep 5
sudo systemctl start llmui-proxy

# 3. V√©rifier les logs Nginx
sudo tail -f /var/log/nginx/llmui-error.log

# 4. Test direct du backend
curl http://localhost:5000/api/health
```

### Erreur 504 Gateway Timeout

**Sympt√¥me**: Requ√™te prend trop de temps et timeout

**Cause**: Ollama ou consensus prend trop de temps

**Solution**:
```bash
# 1. Augmenter les timeouts Nginx
sudo nano /etc/nginx/sites-available/llmui

# Ajouter dans location /api/:
proxy_connect_timeout 120s;
proxy_send_timeout 120s;
proxy_read_timeout 120s;

# 2. Recharger Nginx
sudo nginx -t && sudo systemctl reload nginx

# 3. Augmenter timeout Ollama dans config.yaml
ollama:
  timeout: 600  # 10 minutes
```

### Erreur 401 Unauthorized

**Sympt√¥me**: API retourne 401 m√™me avec token

**Solution**:
```bash
# 1. V√©rifier le token
curl -H "Authorization: Bearer YOUR_TOKEN" \
  http://localhost:5000/api/auth/verify

# 2. R√©g√©n√©rer un token
curl -X POST http://localhost:5000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"your_password"}'

# 3. V√©rifier la config JWT
cat /opt/llmui-core/config.yaml | grep jwt_secret

# 4. Si secret chang√©, red√©marrer
sudo systemctl restart llmui-backend llmui-proxy
```

### WebSocket se d√©connecte

**Sympt√¥me**: Connexion WebSocket se ferme rapidement

**Solution**:
```bash
# 1. Augmenter timeouts WebSocket dans Nginx
sudo nano /etc/nginx/sites-available/llmui

# Dans location /ws/:
proxy_read_timeout 3600s;
proxy_send_timeout 3600s;

# 2. V√©rifier les logs
sudo journalctl -u llmui-backend -f | grep -i websocket

# 3. Test WebSocket
npm install -g wscat
wscat -c ws://localhost:5000/ws/chat
```

---

## ü§ñ Probl√®mes Ollama

### Ollama ne r√©pond pas

**Sympt√¥me**: `curl http://localhost:11434` √©choue

**Solution**:
```bash
# 1. V√©rifier le service
sudo systemctl status ollama

# 2. Red√©marrer
sudo systemctl restart ollama

# 3. V√©rifier les logs
sudo journalctl -u ollama -n 50

# 4. Port en √©coute?
sudo ss -tlnp | grep 11434

# 5. Test API
curl http://localhost:11434/api/tags
```

### Mod√®le introuvable

**Sympt√¥me**: `Error: model 'phi3:3.8b' not found`

**Solution**:
```bash
# Lister les mod√®les
ollama list

# Re-t√©l√©charger
ollama pull phi3:3.8b
ollama pull gemma2:2b
ollama pull granite4:micro-h

# V√©rifier
ollama list
```

### Ollama est lent

**Sympt√¥me**: G√©n√©ration prend >30 secondes

**Causes**:
- Pas de GPU (utilise CPU)
- RAM insuffisante
- Swap excessif

**Solution**:
```bash
# V√©rifier utilisation ressources
top -bn1 | grep ollama
free -h

# Si swap utilis√©
sudo swapoff -a
sudo swapon -a

# R√©duire charge
ollama ps  # Voir mod√®les charg√©s
# Les mod√®les se d√©chargent automatiquement apr√®s 5 min

# Configurer GPU si disponible
# Ajouter dans /etc/systemd/system/ollama.service:
[Service]
Environment="CUDA_VISIBLE_DEVICES=0"

sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### Erreur CUDA/GPU

**Sympt√¥me**: `CUDA error: out of memory`

**Solution**:
```bash
# V√©rifier GPU
nvidia-smi

# R√©duire nombre de mod√®les simultan√©s
# Modifier config.yaml pour utiliser 1 worker au lieu de 2

# Ou forcer CPU
sudo systemctl stop ollama
OLLAMA_FORCE_CPU=1 sudo -u ollama ollama serve
```

---

## üíæ Erreurs de base de donn√©es

### SQLite locked

**Sympt√¥me**: `database is locked`

**Solution**:
```bash
# 1. Identifier les processus utilisant la DB
sudo lsof /opt/llmui-core/data/*.db

# 2. Arr√™ter les services
sudo systemctl stop llmui-backend llmui-proxy

# 3. V√©rifier int√©grit√©
sqlite3 /opt/llmui-core/data/llmui.db "PRAGMA integrity_check;"

# 4. Si OK, red√©marrer
sudo systemctl start llmui-backend llmui-proxy
```

### Base de donn√©es corrompue

**Sympt√¥me**: `database disk image is malformed`

**Solution**:
```bash
# 1. Arr√™ter les services
sudo systemctl stop llmui-backend llmui-proxy

# 2. Backup
cp /opt/llmui-core/data/llmui.db /opt/llmui-core/data/llmui.db.corrupt

# 3. Exporter donn√©es
sqlite3 /opt/llmui-core/data/llmui.db.corrupt ".dump" > /tmp/dump.sql

# 4. Cr√©er nouvelle DB
rm /opt/llmui-core/data/llmui.db
sqlite3 /opt/llmui-core/data/llmui.db < /tmp/dump.sql

# 5. V√©rifier
sqlite3 /opt/llmui-core/data/llmui.db "PRAGMA integrity_check;"

# 6. Red√©marrer
sudo systemctl start llmui-backend llmui-proxy
```

### Erreur migration

**Sympt√¥me**: `no such table: conversations`

**Solution**:
```bash
# R√©initialiser la DB (PERTE DE DONN√âES!)
sudo systemctl stop llmui-backend llmui-proxy

# Backup
sudo cp /opt/llmui-core/data/llmui.db /opt/llmui-core/backups/

# Supprimer
sudo rm /opt/llmui-core/data/llmui.db

# Red√©marrer (cr√©e automatiquement)
sudo systemctl start llmui-backend

# V√©rifier les logs
sudo journalctl -u llmui-backend -n 20
```

---

## üêå Probl√®mes de performance

### R√©ponses lentes

**Diagnostic**:
```bash
# 1. Utilisation CPU/RAM
top
htop

# 2. I/O disque
iostat -x 1

# 3. Temps de r√©ponse API
time curl http://localhost:5000/api/health

# 4. Logs
sudo journalctl -u llmui-backend | grep "processing_time"
```

**Solutions**:

#### Cache
```yaml
# Dans config.yaml
caching:
  enabled: true
  response_cache_size: 2000
  embedding_cache_size: 10000
```

#### Optimiser Ollama
```bash
# Moins de workers
# config.yaml:
ollama:
  models:
    workers:
      - "phi3:3.8b"  # Un seul au lieu de 2
```

#### Base de donn√©es
```bash
# Vacuum SQLite
sqlite3 /opt/llmui-core/data/llmui.db "VACUUM;"

# Nettoyer vieux messages
sqlite3 /opt/llmui-core/data/llmui.db "DELETE FROM messages WHERE created_at < datetime('now', '-30 days');"
```

### M√©moire satur√©e

**Sympt√¥me**: `MemoryError` ou syst√®me swap excessif

**Solution**:
```bash
# Identifier consommation
ps aux --sort=-%mem | head -10

# Limiter m√©moire Ollama
sudo systemctl edit ollama
# Ajouter:
[Service]
MemoryMax=4G

sudo systemctl daemon-reload
sudo systemctl restart ollama

# Nettoyer cache
sudo systemctl restart llmui-backend
```

---

## üåê Probl√®mes r√©seau

### Ne peut pas se connecter depuis l'ext√©rieur

**Sympt√¥me**: Fonctionne sur localhost mais pas depuis autre machine

**Solution**:
```bash
# 1. Pare-feu
sudo ufw status
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# OU
sudo firewall-cmd --list-all
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --permanent --add-service=https
sudo firewall-cmd --reload

# 2. Nginx √©coute sur toutes interfaces?
sudo netstat -tlnp | grep :80

# 3. IP publique
curl ifconfig.me

# 4. Test depuis externe
curl -I http://VOTRE_IP/
```

### SSL/HTTPS ne fonctionne pas

**Sympt√¥me**: Erreur certificat ou connexion refus√©e sur 443

**Solution**:
```bash
# 1. V√©rifier certificats
sudo ls -la /opt/llmui-core/ssl/

# 2. G√©n√©rer avec Let's Encrypt
sudo certbot --nginx -d votre-domaine.com

# 3. Tester renouvellement
sudo certbot renew --dry-run

# 4. V√©rifier config Nginx
sudo nginx -t

# 5. Red√©marrer
sudo systemctl restart nginx
```

---

## üé® Erreurs frontend

### Page blanche

**Sympt√¥me**: Interface ne charge pas, page blanche

**Solution**:
```bash
# 1. Console navigateur (F12)
# Chercher erreurs JavaScript

# 2. V√©rifier fichiers web
ls -la /opt/llmui-core/web/

# 3. Permissions
sudo chown -R www-data:www-data /opt/llmui-core/web/

# 4. Test fichiers
curl http://localhost/
curl http://localhost/index.html
curl http://localhost/app.js

# 5. Logs Nginx
sudo tail -f /var/log/nginx/llmui-error.log
```

### Erreur CORS

**Sympt√¥me**: `Access-Control-Allow-Origin` dans console

**Solution**:
```bash
# Dans llmui_backend.py, v√©rifier CORS:
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # En dev seulement!
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# Red√©marrer
sudo systemctl restart llmui-backend
```

### Fichiers statiques 404

**Sympt√¥me**: CSS/JS ne chargent pas (404)

**Solution**:
```bash
# V√©rifier config Nginx
sudo nano /etc/nginx/sites-available/llmui

# Doit avoir:
root /opt/llmui-core/web;

# Tester
curl -I http://localhost/app.js
curl -I http://localhost/base.css
```

---

## üìä Logs et monitoring

### Consulter tous les logs

```bash
# Backend (temps r√©el)
sudo journalctl -u llmui-backend -f

# Backend (derni√®res 100 lignes)
sudo journalctl -u llmui-backend -n 100

# Backend (depuis 1h)
sudo journalctl -u llmui-backend --since "1 hour ago"

# Proxy
sudo journalctl -u llmui-proxy -f

# Nginx access
sudo tail -f /var/log/nginx/llmui-access.log

# Nginx errors
sudo tail -f /var/log/nginx/llmui-error.log

# Ollama
sudo journalctl -u ollama -f

# Tous en m√™me temps (multitail)
sudo multitail \
  -l "journalctl -u llmui-backend -f" \
  -l "journalctl -u llmui-proxy -f" \
  -l "tail -f /var/log/nginx/llmui-error.log"
```

### Filtrer les logs

```bash
# Erreurs seulement
sudo journalctl -u llmui-backend -p err -n 50

# Recherche mot-cl√©
sudo journalctl -u llmui-backend | grep -i "error"

# Export logs
sudo journalctl -u llmui-backend --since today > ~/llmui-logs-$(date +%Y%m%d).log
```

### Activer debug

```python
# Dans config.yaml
logging:
  level: "DEBUG"  # Au lieu de INFO

# Red√©marrer
sudo systemctl restart llmui-backend llmui-proxy
```

---

## üÜò R√©initialisation compl√®te

Si rien ne fonctionne, r√©initialisation propre:

```bash
#!/bin/bash
# reset_llmui.sh

echo "‚ö†Ô∏è  R√âINITIALISATION COMPL√àTE - Sauvegarder d'abord!"
read -p "Continuer? (tapez 'OUI'): " confirm
if [ "$confirm" != "OUI" ]; then exit 1; fi

# Backup
sudo tar -czf ~/llmui-backup-$(date +%Y%m%d).tar.gz /opt/llmui-core/data

# Arr√™ter
sudo systemctl stop llmui-backend llmui-proxy nginx

# Nettoyer cache/logs
sudo rm -rf /opt/llmui-core/logs/*
sudo rm -rf /opt/llmui-core/cache/*
sudo rm -f /opt/llmui-core/data/*.db-journal

# R√©installer d√©pendances Python
sudo su - llmui -c "/opt/llmui-core/venv/bin/pip install --force-reinstall -r /opt/llmui-core/requirements.txt"

# Red√©marrer
sudo systemctl start llmui-backend
sleep 5
sudo systemctl start llmui-proxy
sleep 3
sudo systemctl start nginx

# V√©rifier
sudo systemctl status llmui-backend llmui-proxy nginx
curl http://localhost:5000/api/health
```

---

## üìû Support suppl√©mentaire

Si le probl√®me persiste:

1. **Consulter les logs complets**:
```bash
sudo tar -czf ~/llmui-debug-$(date +%Y%m%d).tar.gz \
  /tmp/andy_install.log \
  /var/log/nginx/llmui-*.log \
  <(journalctl -u llmui-backend -n 500) \
  <(journalctl -u llmui-proxy -n 500) \
  <(journalctl -u ollama -n 500)
```

2. **Informations syst√®me**:
```bash
uname -a
python3 --version
ollama --version
nginx -v
cat /etc/os-release
df -h
free -h
```

3. **Cr√©er un issue GitHub** avec:
   - Description du probl√®me
   - √âtapes pour reproduire
   - Logs pertinents
   - Informations syst√®me

---

**Francois Chalut**  
*Support technique pour la souverainet√© num√©rique* üá®üá¶

**Derni√®re mise √† jour**: 2025-11-21
